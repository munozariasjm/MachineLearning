{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Regression\n",
    "## OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todavía tenemos el mismo problema de un Data-set de la forma $\\big\\{(x^{(1)},y^{(1)})...(x^{(i)},y^{(i)})\\big\\}$, donde $x^{(i)} \\in \\mathbb{R}^d$, pero a diferencia de los valores de classification, tendremos $y \\in \\mathbb{R}$. Vamos en qué consiste una aproximación al problema mediante Ordinary Least Square (OLS).\n",
    "\n",
    "####  Hypothesis class\n",
    "\n",
    "Queremos hacer una predicción para el valor de $y$, para esto usamos $\\Theta=\\{\\theta \\in \\mathbb{R}^d\\;,\\; \\; \\theta_0 \\in \\mathbb{R}\\}$. \n",
    "Con esto, para hacer la predicción para un cierto $x$, vamos a utilizar:\n",
    "\n",
    "$$h(x;\\Theta)=\\theta^{T}x+\\theta_0$$\n",
    "\n",
    "\n",
    "####  Loss Function\n",
    "\n",
    "Es válido elegir cualquier función para ver qué tan insatisfechos estamos con determinada predicción, no obstante, una de las más famosas es $\\textbf{Regression square loss}$ que tiene la forma:\n",
    "$$\\mathcal{L}(actual, prediction)=(actual-prediction)^2$$\n",
    "$$$$\n",
    "Vamos a tratar la regresión como un problema de optimización, si definimos la función como $$J(\\theta , \\theta _0) = \\frac{1}{n}\\sum _{i = 1}^ n\\left(\\theta ^ Tx^{(i)} + \\theta _0 - y^{(i)}\\right)^2 \\; \\; $$ entonces nos interesa los parámetros que hagan: $\\theta ^*, \\theta _0^* = {\\rm arg}\\min _{\\theta , \\theta _0} J(\\theta , \\theta _0)\\; \\; $.\n",
    "\n",
    "### Analítical Solution\n",
    "\n",
    "Para optimizar, vamos a utilizar la idea de cálculo de derivar con respecto a $\\Theta$, esto nos va a dar un sistema de ocuaciones que vamos a condensar en una matriz que tiene forma:\n",
    "\n",
    "$$X = \\begin{bmatrix} x_1^{(1)} &  \\dots &  x_1^{(n)}\\\\ \\vdots &  \\ddots &  \\vdots \\\\ x_ d^{(1)} &  \\dots &  x_ d^{(n)}\\end{bmatrix} \\; \\; \\; ;  \\; \\; \\;  Y = \\begin{bmatrix} y^{(1)} &  \\dots &  y^{(n)}\\end{bmatrix}\\; \\; .$$\n",
    "\n",
    "Con ello definimos $T$ y $W$ como las transpuestas:\n",
    "\n",
    "$$W = X^ T = \\begin{bmatrix} x_1^{(1)} &  \\dots &  x_ d^{(1)}\\\\ \\vdots &  \\ddots &  \\vdots \\\\ x_1^{(n)} &  \\dots &  x_ d^{(n)}\\end{bmatrix} \\; \\; ; \\; \\; \\;  T = Y^ T = \\begin{bmatrix} y^{(1)}\\\\ \\vdots \\\\ y^{(n)}\\end{bmatrix} \\; \\; .$$\n",
    "\n",
    "\n",
    "Ahora, vamos a poder reescribir la función objetivo en términos de las nuevas matrices nuevas:\n",
    "$$\n",
    "J(\\theta ) = \\frac{1}{n}\\underbrace{(W\\Theta - T)^ T}_{1 \\times n}\\underbrace{(W\\Theta - T)}_{n \\times 1} = \\frac{1}{n}\\sum _{i=1}^ n \\left(\\left(\\sum _{j=1}^ d W_{ij}\\Theta _ j \\right) - T_ i\\right)^2$$.\n",
    "\n",
    "Con esto, la derivada que estabamos buscando tiene la forma de un gradiente \n",
    "$$\\nabla _{\\theta }J = \\frac{2}{n}\\underbrace{W^ T}_{d \\times n}\\underbrace{(W\\theta - T)}_{n \\times 1}\\; \\; $$\n",
    "\n",
    "con esta optimización quedamos al final simplemente con \n",
    "$$W^T \\cdot W \\cdot \\theta-W^T\\cdot T=0 \\; \\; \\; \\; \\iff \\;\\;\\;\\; \\theta=(W^TW)^{-1}WT$$\n",
    "\n",
    "Esto se conoce en la jega matemática como $\\textbf{closed form OLS solution}$.\n",
    "\n",
    "Primero nos tenemos que preocupar con algunos riesgos relacionados con que $W$ no sea invertible y después con el gran costo computacional relacionado con invertir una matriz. \n",
    "\n",
    "## Regularization: Ridge Regression\n",
    "A pesar de no ser la única, vamos a usar una función regularizadora conocida como $\\textit{Ridge Regression}$:\n",
    "$$\n",
    "J_{\\text {ridge}}(\\theta , \\theta _0) = \\frac{1}{n}\\sum _{i = 1}^ n\\left(\\theta ^ Tx^{(i)} + \\theta _0 - y^{(i)}\\right)^2 + \\lambda \\| \\theta \\| ^2$$\n",
    "\n",
    "Esto no sólo evita problemas relacionados con que $W$ no sea invertible, sino también overfitting. Latimosamente optmizar usando $\\theta_0$ es difícil, por esta razón vamos a ignorar temporalmente el offset y si es necesario, depués lo devolvemos. Teniendo esto en cuenta, para optimizar la función objetivo:\n",
    "$$\\nabla _{\\theta }J_\\text {ridge} = \\frac{2}{n}W^ T(W\\theta - T) + 2 \\lambda \\theta \\; \\;=\\frac{2}{n}\\sum _{i = 1}^ n\\left(\\theta ^ Tx^{(i)} + \\theta _0 - y^{(i)}\\right)x^{(i)} + 2\\lambda \\theta=0 .$$\n",
    "\n",
    "$$\\theta^*_{\\text{ridge}}=(W^ TW + n \\lambda I)^{-1}W^ TT$$\n",
    "\n",
    "\n",
    "Acá, $\\lambda$ da información acerca de qué tan fuerte estamos regularizando e $I$ es la matriz identidad.\n",
    "\n",
    "A esta función se le puede aplicar gradient descent, y a pesar de todo, en este preceso es costoso ir calculando los gradientes.\n",
    "Si se tiene un data-set demasiado grande, conviene un poco más utilizar stochastic gradient descent.\n",
    "\n",
    "* Stochastic Gradient Descent: (Se puede usar si el gradientetiene forma de suma)\n",
    "\n",
    "    - Tomar un elemento de la sumatoria aletoria.\n",
    "    - Hacer un pequeño paso en esa dirección, el tamaño del paso debe ser inversamente proporcional al número de pasos dados. ($1/t$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In all the following definitions:\n",
    "# x is d by n : input data\n",
    "# y is 1 by n : output regression values\n",
    "# th is d by 1 : weights\n",
    "# th0 is 1 by 1 or scalar\n",
    "import numpy as np\n",
    "def lin_reg(x, th, th0):\n",
    "    return np.dot(th.T, x) + th0\n",
    "\n",
    "def square_loss(x, y, th, th0):\n",
    "    return (y - lin_reg(x, th, th0))**2\n",
    "\n",
    "def mean_square_loss(x, y, th, th0):\n",
    "    # the axis=1 and keepdims=True are important when x is a full matrix\n",
    "    return np.mean(square_loss(x, y, th, th0), axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_lin_reg_th(x, th, th0):\n",
    "    \"\"\"returns the gradient of lin_reg(x, th, th0) with respect to th\"\"\"\n",
    "    return x\n",
    "\n",
    "def d_square_loss_th(x, y, th, th0):\n",
    "    \"\"\"returns the gradient of square_loss(x, y, th, th0) with respect to th.\"\"\"\n",
    "    return -2 * (y - lin_reg(x, th, th0)) * d_lin_reg_th(x, th, th0)\n",
    "\n",
    "def d_mean_square_loss_th(x, y, th, th0):\n",
    "    \"\"\"returns the gradient of mean_square_loss(x, y, th, th0) with respect to th.\"\"\"\n",
    "    return np.mean(d_square_loss_th(x, y, th, th0), axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_lin_reg_th0(x, th, th0):\n",
    "    \"\"\"returns the gradient of lin_reg(x, th, th0) with respect to th0\"\"\"\n",
    "    return np.ones((1, x.shape[1]))\n",
    "\n",
    "def d_square_loss_th0(x, y, th, th0):\n",
    "    \"\"\"returns the gradient of square_loss(x, y, th, th0) with respect to th0.\"\"\"\n",
    "    return -2 * (y - lin_reg(x, th, th0)) * d_lin_reg_th0(x, th, th0)\n",
    "\n",
    "def d_mean_square_loss_th0(x, y, th, th0):\n",
    "    \"\"\"returns the gradient of mean_square_loss(x, y, th, th0) with respect to th0.\"\"\"\n",
    "    return np.mean(d_square_loss_th0(x, y, th, th0), axis= 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar el ridge objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is d by n : input data\n",
    "# y is 1 by n : output regression values\n",
    "# th is d by 1 : weights\n",
    "# th0 is 1 by 1 or scalar\n",
    "def ridge_obj(x, y, th, th0, lam):\n",
    "    return np.mean(square_loss(x, y, th, th0), axis = 1, keepdims = True) + lam * np.linalg.norm(th)**2\n",
    "def d_ridge_obj_th(x, y, th, th0, lam):\n",
    "    return d_mean_square_loss_th(x, y, th, th0) + 2* lam * th\n",
    "\n",
    "def d_ridge_obj_th0(x, y, th, th0, lam):\n",
    "    return d_mean_square_loss_th0(x, y, th, th0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el Stochstic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_grad(f):\n",
    "    def df(x):\n",
    "        g = np.zeros(x.shape)\n",
    "        delta = 0.001\n",
    "        for i in range(x.shape[0]):\n",
    "            xi = x[i,0]\n",
    "            x[i,0] = xi - delta\n",
    "            xm = f(x)\n",
    "            x[i,0] = xi + delta\n",
    "            xp = f(x)\n",
    "            x[i,0] = xi\n",
    "            g[i,0] = (xp - xm)/(2*delta)\n",
    "        return g\n",
    "    return df\n",
    "def sgd(X, y, J, dJ, w0, step_size_fn, max_iter):\n",
    "    \"\"\"\"\n",
    "    INPUTS:\n",
    "    X: a standard data array (d by n)\n",
    "    y: a standard labels row vector (1 by n)\n",
    "    J: a cost function whose input is a data point (a column vector), a label (1 by 1) and a weight vector w (a column vector) (in that order), and which returns a scalar.\n",
    "    dJ: a cost function gradient (corresponding to J) whose input is a data point (a column vector), a label (1 by 1) and a weight vector w (a column vector) (also in that order), and which returns a column vector.\n",
    "    w0: an initial value of weight vector ww, which is a column vector.\n",
    "    step_size_fn: a function that is given the (zero-indexed) iteration index (an integer) and returns a step size.\n",
    "    max_iter: the number of iterations to perform\n",
    "    \n",
    "    OUTPUTS:\n",
    "    w: the value of the weight vector at the final step\n",
    "    fs: the list of values of JJ found during all the iterations\n",
    "    ws: the list of values of ww found during all the iterations\n",
    "    \"\"\"\n",
    "    n = y.shape[1]\n",
    "    prev_w = w0\n",
    "    fs = []; ws = []\n",
    "    np.random.seed(0)\n",
    "    for i in range(max_iter):\n",
    "        j = np.random.randint(n)\n",
    "        Xj = X[:,j:j+1]; yj = y[:,j:j+1]\n",
    "        prev_f, prev_grad = J(Xj, yj, prev_w), dJ(Xj, yj, prev_w)\n",
    "        fs.append(prev_f); ws.append(prev_w)\n",
    "        if i == max_iter - 1:\n",
    "            return prev_w, fs, ws\n",
    "        step = step_size_fn(i)\n",
    "        prev_w = prev_w - step * prev_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
